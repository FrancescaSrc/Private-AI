{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project: Federated Learning with Encrypted Gradient Aggregation\n",
    "\n",
    "This is my implementation of FL, the code is mainly coming from Tutorial 10 on OpenMinded slack website: https://github.com/OpenMined/PySyft/blob/master/examples/tutorials/Part%2010%20-%20Federated%20Learning%20with%20Secure%20Aggregation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0621 14:05:08.907468 56792 hook.py:96] Torch was already hooked... skipping hooking process\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "import syft as sy\n",
    "hook=sy.TorchHook(th)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class Parser:\n",
    "    \"\"\"Parameters for training\"\"\"\n",
    "    def __init__(self):\n",
    "        self.epochs = 10\n",
    "        self.lr = 0.001\n",
    "        self.test_batch_size = 1\n",
    "        self.batch_size = 1\n",
    "        self.log_interval = 10\n",
    "        self.seed = 1\n",
    "    \n",
    "args = Parser()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "kwargs = {}\n",
    "\n",
    "data=th.tensor([[1.,1],[0,1],[1,0],[0,0]],requires_grad=True)\n",
    "target=th.tensor([[1.],[1],[0],[0]],requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = TensorDataset(data, target)\n",
    "train_loader = DataLoader(train, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [0., 0.]], requires_grad=True), tensor([[1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.]], requires_grad=True))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset.tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just, checking what it's in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<StackBackward>)\n",
      "tensor([[1.]], grad_fn=<StackBackward>)\n",
      "tensor([[0.]], grad_fn=<StackBackward>)\n",
      "tensor([[0.]], grad_fn=<StackBackward>)\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (data,target) in enumerate(train_loader):\n",
    "    print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model and the virtual workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn,optim\n",
    "#model\n",
    "model=nn.Linear(2,1)\n",
    "#optimizer used\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#virtual workers\n",
    "bo=sy.VirtualWorker(hook,id='bo')\n",
    "al=sy.VirtualWorker(hook,id='al')\n",
    "jo=sy.VirtualWorker(hook,id='jo')\n",
    "compute_nodes = [bo, al, jo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a distributed dataset, send the data to each worker\n",
    "train_distributed_dataset = []\n",
    "\n",
    "for batch_idx, (data,target) in enumerate(train_loader):\n",
    "    data = data.send(compute_nodes[batch_idx % len(compute_nodes)])\n",
    "    target = target.send(compute_nodes[batch_idx % len(compute_nodes)])\n",
    "    train_distributed_dataset.append((data, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((Wrapper)>[PointerTensor | me:11617755689 -> bo:90933393668],\n",
       "  (Wrapper)>[PointerTensor | me:98177457693 -> bo:44289883049]),\n",
       " ((Wrapper)>[PointerTensor | me:45272425748 -> al:89313412076],\n",
       "  (Wrapper)>[PointerTensor | me:84225090910 -> al:79352695087]),\n",
       " ((Wrapper)>[PointerTensor | me:4333230265 -> jo:10475716448],\n",
       "  (Wrapper)>[PointerTensor | me:78673644835 -> jo:7826465726]),\n",
       " ((Wrapper)>[PointerTensor | me:31811298497 -> bo:9594562841],\n",
       "  (Wrapper)>[PointerTensor | me:96610485666 -> bo:56670613824])]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_distributed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data,target) in enumerate(train_distributed_dataset):\n",
    "        worker = data.location\n",
    "        model.send(worker)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # update the model\n",
    "        pred = model(data)\n",
    "        loss = F.mse_loss(pred.view(-1), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.get()\n",
    "            \n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss = loss.get()\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * data.shape[0], len(train_loader),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    for data, target in test_loader:\n",
    "        #data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.mse_loss(output.view(-1), target, reduction='sum').item() # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}\\n'.format(test_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/4 (0%)]\tLoss: 1.176870\n",
      "Total 0.07 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\France\\temp-conda\\envs\\pysyft\\lib\\site-packages\\syft\\frameworks\\torch\\tensors\\interpreters\\native.py:226: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  response = eval(cmd)(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "\n",
    "for epoch in range(1):\n",
    "    train(epoch)\n",
    "\n",
    "    \n",
    "total_time = time.time() - t\n",
    "print('Total', round(total_time, 2), 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding encrypted aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_dataset = (list(),list(), list())\n",
    "\n",
    "train_distributed_dataset = []\n",
    "\n",
    "for batch_idx, (data,target) in enumerate(train_loader):\n",
    "    data = data.send(compute_nodes[batch_idx % len(compute_nodes)])\n",
    "    target = target.send(compute_nodes[batch_idx % len(compute_nodes)])\n",
    "    remote_dataset[batch_idx % len(compute_nodes)].append((data, target))\n",
    "\n",
    "    \n",
    "#updatin the parameters for each worker\n",
    "def update(data, target, model, optimizer):\n",
    "    model.send(data.location)\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(data)\n",
    "    loss = F.mse_loss(pred.view(-1), target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss = loss.get().item()\n",
    "    return model, loss\n",
    "\n",
    "bo_model = model.copy()\n",
    "al_model = model.copy()\n",
    "jo_model = model.copy()\n",
    "\n",
    "bo_optimizer = optim.SGD(bo_model.parameters(), lr=args.lr)\n",
    "al_optimizer = optim.SGD(al_model.parameters(), lr=args.lr)\n",
    "jo_optimizer = optim.SGD(jo_model.parameters(), lr=args.lr)\n",
    "\n",
    "models = [bo_model, al_model, jo_model]\n",
    "params = [list(bo_model.parameters()), list(al_model.parameters()), list(jo_model.parameters())]\n",
    "optimizers = [bo_optimizer, al_optimizer, jo_optimizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    for data_index in range(len(remote_dataset[0])-1):\n",
    "        # update remote models\n",
    "        tot_loss=[]\n",
    "        for remote_index in range(len(compute_nodes)):\n",
    "            \n",
    "            data, target = remote_dataset[remote_index][data_index]\n",
    "            #print(\"loss of \" + compute_nodes[remote_index].id)\n",
    "            models[remote_index], loss = update(data, target, models[remote_index], optimizers[remote_index])\n",
    "            #print(loss)\n",
    "            tot_loss.append(loss)\n",
    "        print('avg loss', sum(tot_loss)/3)\n",
    "                    \n",
    "\n",
    "    #calculate new aggregate parameters\n",
    "        new_params = list()\n",
    "        for param_i in range(len(params[0])):\n",
    "            spdz_params = list()\n",
    "            # calculate a spread parameters shared between the three workers\n",
    "            for remote_index in range(len(compute_nodes)):\n",
    "                spdz_params.append((params[remote_index][param_i]+0).fix_precision().share(bo, al, jo).get())\n",
    "            \n",
    "            \n",
    "            \n",
    "            # get the sum encripted params from workers and convert them back to float and calculate average\n",
    "            new_param = (spdz_params[0] + spdz_params[1] + spdz_params[2]).get().float_precision()/3\n",
    "            new_params.append(new_param)\n",
    "            \n",
    "            #print(\"new params list\", new_params)\n",
    "\n",
    "            # zero the params\n",
    "        with torch.no_grad():\n",
    "            for model in params:\n",
    "                for param in model:\n",
    "                    param *= 0\n",
    "                \n",
    "\n",
    "            for model in models:\n",
    "                model.get()\n",
    "\n",
    "            #set the new parameters\n",
    "            for remote_index in range(len(compute_nodes)):\n",
    "                for param_index in range(len(params[remote_index])):\n",
    "                    params[remote_index][param_index].set_(new_params[param_index])\n",
    "                    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    models[0].eval()\n",
    "    test_loss = 0\n",
    "    for data, target in test_loader:\n",
    "        output = models[0](data)\n",
    "        test_loss += F.mse_loss(output.view(-1), target, reduction='sum').item() # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}\\n'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "avg loss 0.6913629804427425\n",
      "2\n",
      "avg loss 0.6891131357600292\n",
      "3\n",
      "avg loss 0.6869764604295293\n",
      "4\n",
      "avg loss 0.6847909204661846\n",
      "5\n",
      "avg loss 0.6826648395508528\n",
      "6\n",
      "avg loss 0.6804896506170431\n",
      "7\n",
      "avg loss 0.6783744376152754\n",
      "8\n",
      "avg loss 0.6762095478673776\n",
      "9\n",
      "avg loss 0.6741050584241748\n",
      "10\n",
      "avg loss 0.6719505063568553\n",
      "Total 0.66 s\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    print(epoch)\n",
    "    train(epoch)\n",
    "    #test() don't have a testloader in this example\n",
    "\n",
    "    \n",
    "total_time = time.time() - t\n",
    "print('Total', round(total_time, 2), 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
